# VerbalVector: AI-Powered Communication Analysis

VerbalVector is a web application that leverages local AI models to provide comprehensive feedback on verbal communication skills. Users can upload or record audio, and the system analyzes it to offer insights into clarity, engagement, pacing, vocal variety, and other detailed metrics, along with qualitative feedback generated by a locally fine-tuned Language Model.

## Core Pipeline & Architecture

The system follows this pipeline:

1.  **Audio Input**: Users upload an audio file (e.g., .mp3, .wav) or record audio directly in the browser via a React frontend.
2.  **Backend Processing (Flask API)**:
    *   The audio is sent to a Python Flask backend.
    *   **Speech-to-Text (STT)**: [Whisper](https://github.com/openai/whisper) (locally loaded `base.en` model) transcribes the audio.
    *   **Feature Extraction**: A combination of audio features (e.g., words per minute, pause count, pitch & volume characteristics via Librosa) and text features (e.g., filler word count, sentence length, unique words via NLTK) are extracted from the audio and transcript.
    *   **LLM Analysis**: The transcript and extracted features are fed into a locally hosted, LoRA fine-tuned LLaMA 3 model via [Ollama](https://ollama.com/). The specific model used is `verbalvector-llama3-lora:latest`. This model generates qualitative feedback and scores based on a structured prompt.
    *   **Vector Storage (Partial Implementation)**: Transcript chunks are processed for potential storage in a [ChromaDB](https://www.trychroma.com/) vector database using Sentence Transformers embeddings, aiming to support future RAG (Retrieval Augmented Generation) Q&A features.
3.  **Results Display**: The React frontend receives the transcript, detailed metrics, key scores (calculated from features), and the LLM-generated feedback, and presents them to the user in an interactive interface.

## Key Features

-   **Web-Based Interface**: Easy-to-use React application for audio input and results visualization.
-   **Local First AI**: Utilizes locally running models (Whisper, Ollama with LoRA LLaMA 3) for privacy and cost-effectiveness.
-   **Comprehensive Analysis**: Provides:
    -   Full audio transcript.
    -   Key communication scores (Overall, Clarity, Engagement, Pacing, Vocal Variety) as percentages.
    -   Detailed metrics (Filler Word Count, Words Per Minute, Avg. Sentence Length, Unique Word Count).
    -   Qualitative feedback generated by the fine-tuned LLM.
-   **Modular Python Backend**: Flask API handles file processing and AI pipeline orchestration.
-   **Submodule for Whisper**: Manages the Whisper codebase as a Git submodule.

## Project Structure

```
VerbalVector/
├── analysis_output/       # Default location for generated analysis files (JSONs, feedback text)
├── data/
│   ├── raw/               # Example raw audio recordings (mostly .gitignored)
│   └── uploads/           # Temporary storage for uploaded files
├── frontend/              # React frontend application (Vite + TypeScript)
│   ├── public/
│   └── src/
│       ├── components/    # React components
│       └── ...
├── src/                   # Python backend source code
│   ├── features/          # Feature extraction logic
│   ├── pipelines/         # Main analysis pipeline orchestration
│   └── vector_store/      # Vector database management
├── vector_db/             # Local ChromaDB storage (gitignored)
├── whisper_source/        # Whisper ASR model (Git submodule)
├── api.py                 # Flask backend API definition
├── main.py                # Older CLI script (partially superseded by api.py for web app)
├── requirements.txt       # Python dependencies
├── README.md              # This file
└── .gitignore             # Git ignore file
```

## Getting Started

### Prerequisites

-   Python 3.9+
-   Node.js and npm (for frontend)
-   **Ollama**: Install Ollama from [ollama.com](https://ollama.com/) and ensure it is running.
-   **FFmpeg**: Required for audio processing (`pydub`). Install via your system package manager (e.g., `brew install ffmpeg` on macOS, `sudo apt update && sudo apt install ffmpeg` on Debian/Ubuntu).
-   Git (for cloning and submodules)

### Installation

1.  **Clone the Repository (with Submodules)**:
    ```bash
    git clone --recurse-submodules https://github.com/YashDe-hub/VerbalVector.git
    cd VerbalVector
    ```
    If you've already cloned without `--recurse-submodules`, run:
    ```bash
    git submodule update --init --recursive
    ```

2.  **Setup Python Backend**:
    ```bash
    # Create and activate a virtual environment
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate

    # Install Python dependencies
    pip install -r requirements.txt
    ```

3.  **Pull Required Ollama Model**:
    Make sure Ollama is running, then in your terminal:
    ```bash
    ollama pull verbalvector-llama3-lora:latest
    # Also ensure the base model for the LoRA (e.g., llama3) is available if the LoRA isn't self-contained.
    # The application currently directly calls `verbalvector-llama3-lora:latest`.
    ```

4.  **Setup Frontend**:
    ```bash
    cd frontend
    npm install
    cd .. 
    ```

### Running the Application

1.  **Start the Backend Server**:
    From the project root directory (`VerbalVector/`):
    ```bash
    source venv/bin/activate 
    python api.py
    ```
    The backend will typically run on `http://localhost:5002`.

2.  **Start the Frontend Development Server**:
    In a new terminal, from the project root directory (`VerbalVector/`):
    ```bash
    cd frontend
    npm run dev
    ```
    The frontend will typically run on `http://localhost:5173` (or the next available port). Open this address in your browser.

## Model Details

### Speech-to-Text: OpenAI Whisper

-   **Model Size**: `base.en` (English-only base model) is used by default for a balance of speed and accuracy on local hardware.
-   **Integration**: Used via the official `whisper` Python package, managed as a Git submodule.

### Language Model for Analysis: `verbalvector-llama3-lora:latest`

-   **Base Model**: LLaMA 3 (specific size depends on your fine-tuning).
-   **Fine-tuning**: LoRA (Low-Rank Adaptation) fine-tuned for the specific task of analyzing communication features and generating structured feedback.
-   **Serving**: Hosted locally via Ollama, ensuring privacy and no API costs for inference.
-   **Interaction**: The Flask backend queries this Ollama endpoint with a detailed prompt containing the transcript and extracted audio/text features.

## Future Work

-   **Full RAG Q&A Implementation**: Complete the frontend UI and backend logic for querying the transcript via the vector database.
-   **User Accounts & History**: Allow users to save and track their analysis history.
-   **Enhanced Visualizations**: Add charts or graphs for visualizing trends in detailed metrics.
-   **Refined Scoring & Metrics**: Further tune the normalization and weighting for key scores. Explore additional relevant communication metrics.
-   **Deployment Strategy**: Package the application (e.g., using Docker) for easier deployment and sharing.

## License

MIT License

## Acknowledgements

-   OpenAI for the Whisper model.
-   The Ollama team for enabling easy local LLM execution.
-   Meta for the LLaMA models.
-   The developers of ChromaDB, Sentence Transformers, Flask, React, and other open-source libraries used. 